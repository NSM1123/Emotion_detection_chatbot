{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_path = \"/kaggle/working/emotion_dataset.csv\"\n",
    "\n",
    "# Load dataset or CSV\n",
    "if not os.path.exists(csv_path):\n",
    "    dataset = load_dataset(\"dair-ai/emotion\")\n",
    "    df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "    # Map label ints to strings\n",
    "    label_map = dataset[\"train\"].features[\"label\"].int2str\n",
    "    df[\"emotion\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Add emotion column if missing\n",
    "    if \"emotion\" not in df.columns:\n",
    "        dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")  # just for label mapping\n",
    "        label_map = dataset.features[\"label\"].int2str\n",
    "        df[\"emotion\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Load Emotion Classifiers (Ensemble)\n",
    "ensemble_models = [\n",
    "    pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None),\n",
    "    pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-go-emotion\", top_k=None)\n",
    "]\n",
    "\n",
    "\n",
    "# 2. Load LLaMA\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")  # local path or HuggingFace\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "llama_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3. Load Flan-T5 as fallback\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "flan_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 4. Rule-based fallback\n",
    "template_responses = {\n",
    "    \"joy\": \"That's wonderful to hear! ðŸ˜Š\",\n",
    "    \"sadness\": \"I'm really sorry you're feeling this way. You're not alone â€” I'm here for you. ðŸ’™\",\n",
    "    \"anger\": \"I understand you're upset. It's okay to feel this way â€” want to talk about it?\",\n",
    "    \"fear\": \"It's okay to feel overwhelmed sometimes. Take a breath â€” youâ€™re doing your best. ðŸ§˜\",\n",
    "    \"surprise\": \"Wow, that does sound unexpected. Thanks for sharing it with me!\",\n",
    "    \"love\": \"Thatâ€™s so heartwarming. Itâ€™s beautiful to feel this way. â¤ï¸\",\n",
    "    \"neutral\": \"Thanks for sharing that. I'm listening. ðŸ¤\",\n",
    "    \"excitement\": \"That's amazing! ðŸŽ‰ So happy for you!\"\n",
    "}\n",
    "\n",
    "def generate_with_flan(emotion, user_input):\n",
    "    prompt = f\"The user is feeling {emotion}. They said: '{user_input}'. Reply empathetically.\"\n",
    "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(flan_model.device)\n",
    "    outputs = flan_model.generate(**inputs, max_new_tokens=80)\n",
    "    return flan_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response_with_emotion(response, emotion):\n",
    "    emotion_keywords = {\n",
    "        \"joy\": [\"happy\", \"joy\", \"wonderful\", \"delight\", \"glad\", \"excited\"],\n",
    "        \"sadness\": [\"sorry\", \"sad\", \"lonely\", \"down\", \"blue\", \"hurt\"],\n",
    "        \"anger\": [\"angry\", \"upset\", \"mad\", \"furious\", \"frustrated\"],\n",
    "        \"fear\": [\"scared\", \"afraid\", \"anxious\", \"nervous\", \"worried\"],\n",
    "        \"love\": [\"love\", \"caring\", \"affection\", \"heartwarming\"],\n",
    "        \"surprise\": [\"surprised\", \"unexpected\", \"shocked\"],\n",
    "        \"excitement\": [\"excited\", \"thrilled\", \"pumped\", \"enthusiastic\"],\n",
    "        \"neutral\": [\"okay\", \"understood\", \"noted\", \"alright\"]\n",
    "    }\n",
    "    return any(word in response.lower() for word in emotion_keywords.get(emotion, []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_emotion_pipeline(user_input):\n",
    "    # ---- Step 1: Emotion Detection ----\n",
    "    score_accumulator = defaultdict(list)\n",
    "    for model in ensemble_models:\n",
    "        results = model(user_input)[0]\n",
    "        for r in results:\n",
    "            score_accumulator[r[\"label\"].lower()].append(r[\"score\"])\n",
    "    \n",
    "    averaged = {k: np.mean(v) for k, v in score_accumulator.items()}\n",
    "    top_emotion = max(averaged, key=averaged.get)\n",
    "\n",
    "    # ---- Step 2: LLaMA Empathetic Response ----\n",
    "    llama_prompt = (\n",
    "    \"You are an empathetic chatbot. Below are examples of how you respond to emotions:\\n\"\n",
    "    \"User is feeling sadness. Message: 'I feel so down.' â†’ Reply: 'I'm really sorry you're feeling this way. You're not alone.'\\n\"\n",
    "    \"User is feeling joy. Message: 'I got a promotion!' â†’ Reply: 'That's amazing! ðŸŽ‰ So happy for you!'\\n\"\n",
    "    \"User is feeling fear. Message: 'I'm scared about tomorrow.' â†’ Reply: 'It's okay to feel overwhelmed sometimes. I'm here for you.'\\n\"\n",
    "    f\"User is feeling {top_emotion}. Message: '{user_input}' â†’ Reply:\"\n",
    "        )\n",
    "\n",
    "    llama_ids = llama_tokenizer(llama_prompt, return_tensors=\"pt\", truncation=True).to(llama_model.device)\n",
    "    llama_out = llama_model.generate(**llama_ids, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    llama_response = llama_tokenizer.decode(llama_out[0], skip_special_tokens=True)\n",
    "\n",
    "    # ---- Step 3: Postprocess LLaMA Output ----\n",
    "    reply_parts = llama_response.split(\"Reply:\")\n",
    "    cleaned_llama = reply_parts[-1].strip() if len(reply_parts) > 1 else llama_response.strip()\n",
    "\n",
    "    # ---- Step 4: Rule-based or Flan Fallback ----\n",
    "    # Check if LLaMA output is poor (empty, generic, or unrelated to the emotion)\n",
    "    if (\n",
    "        len(cleaned_llama.strip()) < 5 or\n",
    "        \"reply kindly\" in cleaned_llama.lower() or\n",
    "        (top_emotion not in cleaned_llama.lower() and not validate_response_with_emotion(cleaned_llama, top_emotion))\n",
    "    ):\n",
    "        try:\n",
    "            # Try Flan fallback\n",
    "            final_response = generate_with_flan(top_emotion, user_input)\n",
    "            llama_source = \"ðŸ¤– (Used Flan-T5 fallback)\"\n",
    "        except:\n",
    "            # If Flan also fails, fallback to rule-based\n",
    "            final_response = template_responses.get(top_emotion, \"I'm here for you.\")\n",
    "            llama_source = \"ðŸ“œ (Used Rule-based fallback)\"\n",
    "    else:\n",
    "        final_response = cleaned_llama\n",
    "        llama_source = \"ðŸ¦™ (LLaMA-generated)\"\n",
    "\n",
    "    # ---- Final Output ----\n",
    "    # print(f\"ðŸ“ User Input: {user_input}\\n\")\n",
    "    # print(f\"ðŸ§  Emotion Detected: {top_emotion}\")\n",
    "    # print(f\"{llama_source} Response: {final_response}\")\n",
    "\n",
    "    return {\n",
    "        \"emotion\": top_emotion,\n",
    "        \"response\": final_response,\n",
    "        \"source\": llama_source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chatbot(row):\n",
    "    result = run_full_emotion_pipeline(row['text'])\n",
    "    return pd.Series({\n",
    "        'detected_emotion': result['emotion'],\n",
    "        'generated_response': result['response']\n",
    "    })\n",
    "\n",
    "# Run the chatbot and append results\n",
    "df_results = df.head(100).copy()\n",
    "df_results[['detected_emotion', 'generated_response']] = df_results.apply(apply_chatbot, axis=1)\n",
    "df_results.to_csv(\"/kaggle/working/chatbot_results_improved_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"/kaggle/working/chatbot_results_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dataset[\"test\"].to_pandas()\n",
    "df_test = df_test.head(100).copy()\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def apply_chatbot(row):\n",
    "    result = run_full_emotion_pipeline(row['text'])  # This should return a dict with 'emotion' and 'response'\n",
    "    return pd.Series({\n",
    "        'detected_emotion': result['emotion'],\n",
    "        'generated_response': result['response']\n",
    "    })\n",
    "\n",
    "# Apply chatbot\n",
    "df_test[['detected_emotion', 'generated_response']] = df_test.progress_apply(apply_chatbot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"/kaggle/working/chatbot_test_results_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
